# 1.1 생성 AI 열풍의 주역, LLM

ChatGPT

- 초거대 언어 모델(LLM)
- 트랜스포머(Transformer)라는 자연어 처리 모델

## 규칙 기반 자연어 처리, 최초 AI 챗봇 ELIZA

- 사용자가 입력한 문장에서 키워드를 인식
- 찾아낸 키워드를 바탕으로 정의된 규칙에 따라 입력을 변환하고 응답 생성에 활용

> 자연어 처리(NLP)
> 인간의 언어를 해석, 조작, 이해하는 능력을 컴퓨터에 부여하는 기계학습
>
> 자연어 처리 모델
> 통계적 추론, 머신러닝, 딥러닝 등의 기계학습 기법에 기반하여 자연어 처리를 위해 학습된 기계 학습 모델
> 통계적 추론: N-gram, TF-IDF
> 머신러닝: SVM, 랜덤 포레스트
> 딥러닝: BERT, GPT

## 통계 기반 자연어 처리, N-gram의 등장

HMM: 이전 상태를 기반으로 다음 상태를 예측 하는 마르코프 모델에 은닉 상태를 더한 것, 연속적인 데이터를 처리하는데 강점을 지닌 모델

CRF: HHM과 유사하게 연속적인 데이터를 처리하지만, 각 상태가 이전상태 뿐만 아니라 다른 관측치ㅊ에도 의존할 수 있게 만드는 조건부 확률 모델

희소성 문제 : 학습 데이터에서 관찰되지 않은 단어 조합이나 시퀀스가 테스트 시에 등장할 때 발생

N-gram: 현재 단어를 예측하기 위해 이전 단어 또는 문장을 참고하는 모델, N의 크기를 키울수록 많은 단어를 참고하여 예측하므로 정확도가 높아지지만, 희소성 문제는 더 심해짐

## 딥러닝과 NLP의 발전, CNN & RNN & LSTM

CNN: 합성곱 신경망, 이미지 분류 모델

RNN: 순환 신경망, 연속된(시퀀스) 데이터 처리에 특화된 모델

> 기울기 소실 문제
> 기울기가 0에 가까워져 학습이 정지되는 문제
> 문서가 길어질 경우 과거의 정보가 점차 희미해짐
> 최근 데이터의 중요도가 높아지는 현상

LSTM: 게이트 개념이 추가된 모델, RNN의 단점을 보완한 모델, 장기 의존성 문제를 해결하고 더 나은 성능을 보임

> 게이트 개념
> 입력, 출력, 정보 흐름을 조절하는 게이트 개념이 추가된 모델
> 망각 게이트, 입력 게이트, 출력 게이트
> 망각 게이트: 이전 상태를 잊어버리는 게이트
> 입력 게이트: 새로운 정보를 입력하는 게이트
> 출력 게이트: 출력을 결정하는 게이트

## 언어 모델의 혁신, 트랜스포머

### 인코더-디코더, 시퀀스 투 시퀀스

### 어텐션 메커니즘

Self-Attention
Multi-Head Attention

### 트랜스포머

Multi-Head Attention
Positional Encoding

## Scale is all you need, LLM의 시작

트랜스포머 모델의 Decoder 구조만 반복적으로 활용
문장의 이해와 출력 모두 Decoder 모듈에서 담당
Masked Multi-Self Attention

1. 공부 단계
2. 문제 풀이 단계
3. 시험 단계 : PPO(Proximal Policy Optimization) 알고리즘

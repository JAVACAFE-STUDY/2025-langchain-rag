# RAG으로 다양한 문서 다루기

## 4.1 - Document Loaders 알아보기

### RAG를 위한 Document 객체의 이해

- 1. RAG의 답변 근거

- 2. 메타 데이터 기반 필터링

## 4.2 - PDF 파일을 Document로 불러오기

### PyPDFLoader

드래그해서 긁어올 수 있는 텍스트만 추출 가능
이미지형태 텍스트 + 차트 추출 -> OCR 기능 필요
rapidocr-onnxruntime 라이브러리 사용

### PyPDFium2

OCR 기능은 없지만 빠름

## 4.3 - 여러 파일을 Document로 불러오기

### Word 파일 불러오기, Docx2txtLoader

```python
#Docx2txtLoader 불러오기
from langchain.document_loaders import Docx2txtLoader

#Docx2txtLoader로 워드 파일 불러오기(경로 설정)
loader = Docx2txtLoader(r"../data/[삼성전자] 사업보고서(일반법인) (2021.03.09).docx")

#페이지로 분할하여 불러오기
data = loader.load_and_split()

#첫번째 페이지 출력하기
print(data[12].page_content[:500])
#로드한 워드파일의 메타데이터 확인
print(data[12].metadata)
```

### CSV 파일 불러오기, csv_loader

```python
from langchain.document_loaders.csv_loader import CSVLoader

loader = CSVLoader(file_path=r"../data/mlb_teams_2012.csv")

data = loader.load()

data[0]
```

### PPT 파일 불러오기, UnstructuredPowerPointLoader

```python
import nltk
print(nltk.__version__)

#UnstructuredPowerPointLoader 불러오기
from langchain.document_loaders import UnstructuredPowerPointLoader

#mode=elements를 통해 pptx의 요소별로 Document 객체로 가져오기
loader = UnstructuredPowerPointLoader(r"../data/Copilot-scenarios-for-Marketing.pptx", mode="elements")

#pptx 파일을 분할 로드하기
data = loader.load_and_split()

data[1]

for i in data:
    if i.metadata['page_number'] == 2:
        print(i.metadata['category'])
        print(i.page_content)
        print("\n")

```

### 인터넷 정보 로드하기, WebBaseLoader

```python

from langchain_community.document_loaders import WebBaseLoader
#텍스트 추출할 URL 입력
loader = WebBaseLoader("https://www.espn.com/")
#ssl verification 에러 방지를 위한 코드
loader.requests_kwargs = {'verify':False}
data = loader.load()
data

#[Headline만 가져오기]
import bs4
from langchain_community.document_loaders import WebBaseLoader
#텍스트 추출할 URL 입력
loader = WebBaseLoader("https://www.espn.com/",
                        bs_kwargs=dict(
                            parse_only=bs4.SoupStrainer(
                                class_=("headlineStack top-headlines")
                                                        )
                                        )
                      )
#ssl verification 에러 방지를 위한 코드
loader.requests_kwargs = {'verify':False}
data = loader.load()
data

loader = WebBaseLoader(["https://www.espn.com/", "https://google.com"])
docs = loader.load()
docs
```

### 특정 경로 내의 모든 파일 불러오기, DirectoryLoader

```python
from langchain_community.document_loaders import DirectoryLoader
#첫번째 매개변수로 경로 입력, glob에 해당 경로에서 불러들일 파일의 형식 지정
#*는 모든 문자를 표현하는 와일드카드로, .pdf로 끝나는 모든 파일을 의미함
loader = DirectoryLoader(r'../data/', glob="*.pdf")
docs = loader.load()
[i.metadata['source'] for i in docs]
```

## 4.4 - 문서를 다양하게 자르는 Text Splitters

### 벡터 DB 저장 과정

RAG 시스템 상에서 문서를 불러온 후 해야하는 작업 -> 벡터 DB에 저장

문서가 긴 경우 한꺼번에 문서를 벡터DB로 변환하는 것은 지양해야함

#### 1. 임베딩 모델의 컨텍스트 윈도우 문제

임베딩 모델도 컨텍스트 윈도우 라는 입력 텍스트 길이 제한이 있음

임베딩 모델들의 최대 시퀀스 길이
| 임베딩 모델명 | 최대 입력 길이 |
|--------------|---------------|
| OpenAI text-embedding-ada-002 | 8,191 토큰 |
| BERT | 512 토큰 |
| RoBERTa | 512 토큰 |
| MPNet | 512 토큰 |
| Sentence-BERT | 256-512 토큰 |
| KoSimCSE-BERT | 512 토큰 |
참고사항:
토큰은 모델마다 다르게 계산될 수 있습니다
일반적으로 영어의 경우 1토큰은 약 4글자에 해당합니다
한글은 모델에 따라 1토큰이 1-2글자에 해당할 수 있습니다

#### 2. LLM의 컨텍스트 윈도우 문제

사용자 질문에 답변할 수 있는 근거 문서가 여러 문서에 산재할 수 있음.
이 크기가 LLM의 컨텍스트 윈도우를 넘어서는 안됨

#### 3. '건초더미에서 바늘 찾기' 문제

- 주어진 텍스트에서 특정 위치의 정보를 얼마나 정확하게 찾아내는지 측정
- 컨텍스트 윈도우까지 글의 길이를 늘려가며 측정
- 최대 입력 길이의 토큰이 크더라도, 앞부분 정보는 망각할 가능성이 높음

### 적당한 크기로 문서를 분할하는 Text Splitter

Document를 특정 기준에 따라 정해진 길이의 청크로 분할

### 단순 글자수 기반 문서 분할, CharacterTextSplitter

- 원하는 글자수 대로 분할할 수 있을 것 같지만 실상은 그렇지 않음. 구분자로 텍스트를 분할하기 때문에, 줄바꿈을 하지 않고 한 문장에 500자 넘는 텍스트가 있으면 하나의 청크로 구성하게 됨.
- 사용자가 컨텍스트 윈도우를 고려하여 chunk_size를 엄격하게 관리하고 싶을 경우 단점이될 수 있음.

```python
# #Langchain Text Splitter 모듈 다운로드
# !pip install -qU langchain langchain-community langchain-text-splitters langchain-openai langchain-experimental pypdfium2 pypdf
```

```python
#PyPDFium2Loader로 PDF 문서 로드하기
from langchain.document_loaders import PyPDFium2Loader
loader = PyPDFium2Loader(r"../data/[이슈리포트 2022-2호] 혁신성장 정책금융 동향.pdf")
pages = loader.load()

#CharacterTextSplitter 모듈 로드
from langchain_text_splitters import CharacterTextSplitter

#구분자: 줄넘김, 청크 길이: 500, 청크 오버랩: 100, length_function: 글자수
text_splitter = CharacterTextSplitter(
    separator="\n",
    chunk_size=500,
    chunk_overlap=100,
    length_function=len
)
#텍스트 분할
texts = text_splitter.split_documents(pages)
print(texts[0])
```

```python
loader = PyPDFium2Loader(r"../data/[이슈리포트 2022-2호] 혁신성장 정책금융 동향.pdf")

pages = loader.load()

from langchain_text_splitters import CharacterTextSplitter

text_splitter = CharacterTextSplitter(
    separator="\n\n",
    chunk_size=500,
    chunk_overlap=100,
    length_function=len,
    is_separator_regex=False,
)

texts = text_splitter.split_documents(pages)
print([len(i.page_content) for i in texts])
```

### 재귀적 문서 분할, RecursiveCharacterTextSplitter

- 여러 개의 구분자를 재귀적으로 적용하여 지정된 청크 길이를 지킬 수 있도록텍스트를 분할함
- [chunkviz](https://chunkviz.up.railway.app/) 사이트 이용
  - 실제 수행 결과를 자세히 확인할 수 있음

```python
from langchain.document_loaders import PyPDFium2Loader

loader = PyPDFium2Loader(r"../data/[이슈리포트 2022-2호] 혁신성장 정책금융 동향.pdf")

pages = loader.load()

from langchain_text_splitters import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter (
    separators=["\n\n", "\n", " ", ""],
    chunk_size=500,
    chunk_overlap=100,
    length_function=len,
    is_separator_regex=False,
)

texts = text_splitter.split_documents(pages)
print([len(i.page_content) for i in texts])
```

### 문맥 파악 통한 문서 분할, Semantic Chunker

- 앞선 chunker들 처럼 기계적 분할로 인한 문서의 맥락이 끊어짐을 방지
- 같은 맥락의 문장은 하나의 청크로 담고, 맥락이 달라지는 경우 다른 청크로 분할
  - 문장 간의 거리가 이상 값(Outlier)에 해당할 정도로 높은 지점에서 문장을 분리
  - 문장 간의 거리는 문장을 임베딩하여 임베딩 값 사이의 거리 측정을 통해 알아내고, 이상 값은 백분위로 설정하여 찾아내게됨. 문장간의 거리를 알기 위해 문장마다 임베딩을 거침
- BERT 같은 임베딩 모델을 이용하면 문장을 행렬 형태의 수치 데이터로 변환할 수 있음
- 실제로 문장들의 N개의 문장을 Window로 삼고 이 Window를 점차 우측으로 옮겨가며 그룹 간의 거리를 측정하는 방식, 문장 간의 거리가 멀어지는 지점이 있고 맥락이 변화하는 것을 파악함
- 청크 사이즈는 제각각 다양한 길이를 가질 수 있음

```python
from langchain.document_loaders import PyPDFLoader

loader = PyPDFLoader(r"../data/BOK 이슈노트 제2022-38호 인공지능 언어모형을 이용한 인플레이션 어조지수 개발 및 시사점.pdf")
pages = loader.load_and_split()

from langchain_experimental.text_splitter import SemanticChunker
from langchain_openai.embeddings import OpenAIEmbeddings
text_splitter = SemanticChunker(OpenAIEmbeddings(openai_api_key = "YOUR_OPENAI_API_KEY"))

texts = text_splitter.split_documents(pages)
print("-"*100)
print("[첫번째 청크]")
print(texts[0].page_content)
print("-"*100)
print("[두번째 청크]")
print(texts[1].page_content)

```
